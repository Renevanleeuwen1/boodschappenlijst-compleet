{"ast":null,"code":"// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\nimport { APIResource } from \"../../core/resource.mjs\";\nimport { CursorPage } from \"../../core/pagination.mjs\";\nimport { buildHeaders } from \"../../internal/headers.mjs\";\nimport { sleep } from \"../../internal/utils/sleep.mjs\";\nimport { allSettledWithThrow } from \"../../lib/Util.mjs\";\nimport { path } from \"../../internal/utils/path.mjs\";\nexport class FileBatches extends APIResource {\n  /**\r\n   * Create a vector store file batch.\r\n   */\n  create(vectorStoreID, body, options) {\n    return this._client.post(path`/vector_stores/${vectorStoreID}/file_batches`, {\n      body,\n      ...options,\n      headers: buildHeaders([{\n        'OpenAI-Beta': 'assistants=v2'\n      }, options?.headers])\n    });\n  }\n  /**\r\n   * Retrieves a vector store file batch.\r\n   */\n  retrieve(batchID, params, options) {\n    const {\n      vector_store_id\n    } = params;\n    return this._client.get(path`/vector_stores/${vector_store_id}/file_batches/${batchID}`, {\n      ...options,\n      headers: buildHeaders([{\n        'OpenAI-Beta': 'assistants=v2'\n      }, options?.headers])\n    });\n  }\n  /**\r\n   * Cancel a vector store file batch. This attempts to cancel the processing of\r\n   * files in this batch as soon as possible.\r\n   */\n  cancel(batchID, params, options) {\n    const {\n      vector_store_id\n    } = params;\n    return this._client.post(path`/vector_stores/${vector_store_id}/file_batches/${batchID}/cancel`, {\n      ...options,\n      headers: buildHeaders([{\n        'OpenAI-Beta': 'assistants=v2'\n      }, options?.headers])\n    });\n  }\n  /**\r\n   * Create a vector store batch and poll until all files have been processed.\r\n   */\n  async createAndPoll(vectorStoreId, body, options) {\n    const batch = await this.create(vectorStoreId, body);\n    return await this.poll(vectorStoreId, batch.id, options);\n  }\n  /**\r\n   * Returns a list of vector store files in a batch.\r\n   */\n  listFiles(batchID, params, options) {\n    const {\n      vector_store_id,\n      ...query\n    } = params;\n    return this._client.getAPIList(path`/vector_stores/${vector_store_id}/file_batches/${batchID}/files`, CursorPage, {\n      query,\n      ...options,\n      headers: buildHeaders([{\n        'OpenAI-Beta': 'assistants=v2'\n      }, options?.headers])\n    });\n  }\n  /**\r\n   * Wait for the given file batch to be processed.\r\n   *\r\n   * Note: this will return even if one of the files failed to process, you need to\r\n   * check batch.file_counts.failed_count to handle this case.\r\n   */\n  async poll(vectorStoreID, batchID, options) {\n    const headers = buildHeaders([options?.headers, {\n      'X-Stainless-Poll-Helper': 'true',\n      'X-Stainless-Custom-Poll-Interval': options?.pollIntervalMs?.toString() ?? undefined\n    }]);\n    while (true) {\n      const {\n        data: batch,\n        response\n      } = await this.retrieve(batchID, {\n        vector_store_id: vectorStoreID\n      }, {\n        ...options,\n        headers\n      }).withResponse();\n      switch (batch.status) {\n        case 'in_progress':\n          let sleepInterval = 5000;\n          if (options?.pollIntervalMs) {\n            sleepInterval = options.pollIntervalMs;\n          } else {\n            const headerInterval = response.headers.get('openai-poll-after-ms');\n            if (headerInterval) {\n              const headerIntervalMs = parseInt(headerInterval);\n              if (!isNaN(headerIntervalMs)) {\n                sleepInterval = headerIntervalMs;\n              }\n            }\n          }\n          await sleep(sleepInterval);\n          break;\n        case 'failed':\n        case 'cancelled':\n        case 'completed':\n          return batch;\n      }\n    }\n  }\n  /**\r\n   * Uploads the given files concurrently and then creates a vector store file batch.\r\n   *\r\n   * The concurrency limit is configurable using the `maxConcurrency` parameter.\r\n   */\n  async uploadAndPoll(vectorStoreId, {\n    files,\n    fileIds = []\n  }, options) {\n    if (files == null || files.length == 0) {\n      throw new Error(`No \\`files\\` provided to process. If you've already uploaded files you should use \\`.createAndPoll()\\` instead`);\n    }\n    const configuredConcurrency = options?.maxConcurrency ?? 5;\n    // We cap the number of workers at the number of files (so we don't start any unnecessary workers)\n    const concurrencyLimit = Math.min(configuredConcurrency, files.length);\n    const client = this._client;\n    const fileIterator = files.values();\n    const allFileIds = [...fileIds];\n    // This code is based on this design. The libraries don't accommodate our environment limits.\n    // https://stackoverflow.com/questions/40639432/what-is-the-best-way-to-limit-concurrency-when-using-es6s-promise-all\n    async function processFiles(iterator) {\n      for (let item of iterator) {\n        const fileObj = await client.files.create({\n          file: item,\n          purpose: 'assistants'\n        }, options);\n        allFileIds.push(fileObj.id);\n      }\n    }\n    // Start workers to process results\n    const workers = Array(concurrencyLimit).fill(fileIterator).map(processFiles);\n    // Wait for all processing to complete.\n    await allSettledWithThrow(workers);\n    return await this.createAndPoll(vectorStoreId, {\n      file_ids: allFileIds\n    });\n  }\n}","map":{"version":3,"names":["APIResource","CursorPage","buildHeaders","sleep","allSettledWithThrow","path","FileBatches","create","vectorStoreID","body","options","_client","post","headers","retrieve","batchID","params","vector_store_id","get","cancel","createAndPoll","vectorStoreId","batch","poll","id","listFiles","query","getAPIList","pollIntervalMs","toString","undefined","data","response","withResponse","status","sleepInterval","headerInterval","headerIntervalMs","parseInt","isNaN","uploadAndPoll","files","fileIds","length","Error","configuredConcurrency","maxConcurrency","concurrencyLimit","Math","min","client","fileIterator","values","allFileIds","processFiles","iterator","item","fileObj","file","purpose","push","workers","Array","fill","map","file_ids"],"sources":["C:\\boodschappenlijst\\boodschappenlijst-nieuw\\node_modules\\openai\\src\\resources\\vector-stores\\file-batches.ts"],"sourcesContent":["// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\r\n\r\nimport { APIResource } from '../../core/resource';\r\nimport * as FilesAPI from './files';\r\nimport { VectorStoreFilesPage } from './files';\r\nimport * as VectorStoresAPI from './vector-stores';\r\nimport { APIPromise } from '../../core/api-promise';\r\nimport { CursorPage, type CursorPageParams, PagePromise } from '../../core/pagination';\r\nimport { buildHeaders } from '../../internal/headers';\r\nimport { RequestOptions } from '../../internal/request-options';\r\nimport { sleep } from '../../internal/utils/sleep';\r\nimport { type Uploadable } from '../../uploads';\r\nimport { allSettledWithThrow } from '../../lib/Util';\r\nimport { path } from '../../internal/utils/path';\r\n\r\nexport class FileBatches extends APIResource {\r\n  /**\r\n   * Create a vector store file batch.\r\n   */\r\n  create(\r\n    vectorStoreID: string,\r\n    body: FileBatchCreateParams,\r\n    options?: RequestOptions,\r\n  ): APIPromise<VectorStoreFileBatch> {\r\n    return this._client.post(path`/vector_stores/${vectorStoreID}/file_batches`, {\r\n      body,\r\n      ...options,\r\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\r\n    });\r\n  }\r\n\r\n  /**\r\n   * Retrieves a vector store file batch.\r\n   */\r\n  retrieve(\r\n    batchID: string,\r\n    params: FileBatchRetrieveParams,\r\n    options?: RequestOptions,\r\n  ): APIPromise<VectorStoreFileBatch> {\r\n    const { vector_store_id } = params;\r\n    return this._client.get(path`/vector_stores/${vector_store_id}/file_batches/${batchID}`, {\r\n      ...options,\r\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\r\n    });\r\n  }\r\n\r\n  /**\r\n   * Cancel a vector store file batch. This attempts to cancel the processing of\r\n   * files in this batch as soon as possible.\r\n   */\r\n  cancel(\r\n    batchID: string,\r\n    params: FileBatchCancelParams,\r\n    options?: RequestOptions,\r\n  ): APIPromise<VectorStoreFileBatch> {\r\n    const { vector_store_id } = params;\r\n    return this._client.post(path`/vector_stores/${vector_store_id}/file_batches/${batchID}/cancel`, {\r\n      ...options,\r\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\r\n    });\r\n  }\r\n\r\n  /**\r\n   * Create a vector store batch and poll until all files have been processed.\r\n   */\r\n  async createAndPoll(\r\n    vectorStoreId: string,\r\n    body: FileBatchCreateParams,\r\n    options?: RequestOptions & { pollIntervalMs?: number },\r\n  ): Promise<VectorStoreFileBatch> {\r\n    const batch = await this.create(vectorStoreId, body);\r\n    return await this.poll(vectorStoreId, batch.id, options);\r\n  }\r\n\r\n  /**\r\n   * Returns a list of vector store files in a batch.\r\n   */\r\n  listFiles(\r\n    batchID: string,\r\n    params: FileBatchListFilesParams,\r\n    options?: RequestOptions,\r\n  ): PagePromise<VectorStoreFilesPage, FilesAPI.VectorStoreFile> {\r\n    const { vector_store_id, ...query } = params;\r\n    return this._client.getAPIList(\r\n      path`/vector_stores/${vector_store_id}/file_batches/${batchID}/files`,\r\n      CursorPage<FilesAPI.VectorStoreFile>,\r\n      { query, ...options, headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]) },\r\n    );\r\n  }\r\n\r\n  /**\r\n   * Wait for the given file batch to be processed.\r\n   *\r\n   * Note: this will return even if one of the files failed to process, you need to\r\n   * check batch.file_counts.failed_count to handle this case.\r\n   */\r\n  async poll(\r\n    vectorStoreID: string,\r\n    batchID: string,\r\n    options?: RequestOptions & { pollIntervalMs?: number },\r\n  ): Promise<VectorStoreFileBatch> {\r\n    const headers = buildHeaders([\r\n      options?.headers,\r\n      {\r\n        'X-Stainless-Poll-Helper': 'true',\r\n        'X-Stainless-Custom-Poll-Interval': options?.pollIntervalMs?.toString() ?? undefined,\r\n      },\r\n    ]);\r\n\r\n    while (true) {\r\n      const { data: batch, response } = await this.retrieve(\r\n        batchID,\r\n        { vector_store_id: vectorStoreID },\r\n        {\r\n          ...options,\r\n          headers,\r\n        },\r\n      ).withResponse();\r\n\r\n      switch (batch.status) {\r\n        case 'in_progress':\r\n          let sleepInterval = 5000;\r\n\r\n          if (options?.pollIntervalMs) {\r\n            sleepInterval = options.pollIntervalMs;\r\n          } else {\r\n            const headerInterval = response.headers.get('openai-poll-after-ms');\r\n            if (headerInterval) {\r\n              const headerIntervalMs = parseInt(headerInterval);\r\n              if (!isNaN(headerIntervalMs)) {\r\n                sleepInterval = headerIntervalMs;\r\n              }\r\n            }\r\n          }\r\n          await sleep(sleepInterval);\r\n          break;\r\n        case 'failed':\r\n        case 'cancelled':\r\n        case 'completed':\r\n          return batch;\r\n      }\r\n    }\r\n  }\r\n\r\n  /**\r\n   * Uploads the given files concurrently and then creates a vector store file batch.\r\n   *\r\n   * The concurrency limit is configurable using the `maxConcurrency` parameter.\r\n   */\r\n  async uploadAndPoll(\r\n    vectorStoreId: string,\r\n    { files, fileIds = [] }: { files: Uploadable[]; fileIds?: string[] },\r\n    options?: RequestOptions & { pollIntervalMs?: number; maxConcurrency?: number },\r\n  ): Promise<VectorStoreFileBatch> {\r\n    if (files == null || files.length == 0) {\r\n      throw new Error(\r\n        `No \\`files\\` provided to process. If you've already uploaded files you should use \\`.createAndPoll()\\` instead`,\r\n      );\r\n    }\r\n\r\n    const configuredConcurrency = options?.maxConcurrency ?? 5;\r\n\r\n    // We cap the number of workers at the number of files (so we don't start any unnecessary workers)\r\n    const concurrencyLimit = Math.min(configuredConcurrency, files.length);\r\n\r\n    const client = this._client;\r\n    const fileIterator = files.values();\r\n    const allFileIds: string[] = [...fileIds];\r\n\r\n    // This code is based on this design. The libraries don't accommodate our environment limits.\r\n    // https://stackoverflow.com/questions/40639432/what-is-the-best-way-to-limit-concurrency-when-using-es6s-promise-all\r\n    async function processFiles(iterator: IterableIterator<Uploadable>) {\r\n      for (let item of iterator) {\r\n        const fileObj = await client.files.create({ file: item, purpose: 'assistants' }, options);\r\n        allFileIds.push(fileObj.id);\r\n      }\r\n    }\r\n\r\n    // Start workers to process results\r\n    const workers = Array(concurrencyLimit).fill(fileIterator).map(processFiles);\r\n\r\n    // Wait for all processing to complete.\r\n    await allSettledWithThrow(workers);\r\n\r\n    return await this.createAndPoll(vectorStoreId, {\r\n      file_ids: allFileIds,\r\n    });\r\n  }\r\n}\r\n\r\n/**\r\n * A batch of files attached to a vector store.\r\n */\r\nexport interface VectorStoreFileBatch {\r\n  /**\r\n   * The identifier, which can be referenced in API endpoints.\r\n   */\r\n  id: string;\r\n\r\n  /**\r\n   * The Unix timestamp (in seconds) for when the vector store files batch was\r\n   * created.\r\n   */\r\n  created_at: number;\r\n\r\n  file_counts: VectorStoreFileBatch.FileCounts;\r\n\r\n  /**\r\n   * The object type, which is always `vector_store.file_batch`.\r\n   */\r\n  object: 'vector_store.files_batch';\r\n\r\n  /**\r\n   * The status of the vector store files batch, which can be either `in_progress`,\r\n   * `completed`, `cancelled` or `failed`.\r\n   */\r\n  status: 'in_progress' | 'completed' | 'cancelled' | 'failed';\r\n\r\n  /**\r\n   * The ID of the\r\n   * [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)\r\n   * that the [File](https://platform.openai.com/docs/api-reference/files) is\r\n   * attached to.\r\n   */\r\n  vector_store_id: string;\r\n}\r\n\r\nexport namespace VectorStoreFileBatch {\r\n  export interface FileCounts {\r\n    /**\r\n     * The number of files that where cancelled.\r\n     */\r\n    cancelled: number;\r\n\r\n    /**\r\n     * The number of files that have been processed.\r\n     */\r\n    completed: number;\r\n\r\n    /**\r\n     * The number of files that have failed to process.\r\n     */\r\n    failed: number;\r\n\r\n    /**\r\n     * The number of files that are currently being processed.\r\n     */\r\n    in_progress: number;\r\n\r\n    /**\r\n     * The total number of files.\r\n     */\r\n    total: number;\r\n  }\r\n}\r\n\r\nexport interface FileBatchCreateParams {\r\n  /**\r\n   * A list of [File](https://platform.openai.com/docs/api-reference/files) IDs that\r\n   * the vector store should use. Useful for tools like `file_search` that can access\r\n   * files.\r\n   */\r\n  file_ids: Array<string>;\r\n\r\n  /**\r\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\r\n   * for storing additional information about the object in a structured format, and\r\n   * querying for objects via API or the dashboard. Keys are strings with a maximum\r\n   * length of 64 characters. Values are strings with a maximum length of 512\r\n   * characters, booleans, or numbers.\r\n   */\r\n  attributes?: Record<string, string | number | boolean> | null;\r\n\r\n  /**\r\n   * The chunking strategy used to chunk the file(s). If not set, will use the `auto`\r\n   * strategy. Only applicable if `file_ids` is non-empty.\r\n   */\r\n  chunking_strategy?: VectorStoresAPI.FileChunkingStrategyParam;\r\n}\r\n\r\nexport interface FileBatchRetrieveParams {\r\n  /**\r\n   * The ID of the vector store that the file batch belongs to.\r\n   */\r\n  vector_store_id: string;\r\n}\r\n\r\nexport interface FileBatchCancelParams {\r\n  /**\r\n   * The ID of the vector store that the file batch belongs to.\r\n   */\r\n  vector_store_id: string;\r\n}\r\n\r\nexport interface FileBatchListFilesParams extends CursorPageParams {\r\n  /**\r\n   * Path param: The ID of the vector store that the files belong to.\r\n   */\r\n  vector_store_id: string;\r\n\r\n  /**\r\n   * Query param: A cursor for use in pagination. `before` is an object ID that\r\n   * defines your place in the list. For instance, if you make a list request and\r\n   * receive 100 objects, starting with obj_foo, your subsequent call can include\r\n   * before=obj_foo in order to fetch the previous page of the list.\r\n   */\r\n  before?: string;\r\n\r\n  /**\r\n   * Query param: Filter by file status. One of `in_progress`, `completed`, `failed`,\r\n   * `cancelled`.\r\n   */\r\n  filter?: 'in_progress' | 'completed' | 'failed' | 'cancelled';\r\n\r\n  /**\r\n   * Query param: Sort order by the `created_at` timestamp of the objects. `asc` for\r\n   * ascending order and `desc` for descending order.\r\n   */\r\n  order?: 'asc' | 'desc';\r\n}\r\n\r\nexport declare namespace FileBatches {\r\n  export {\r\n    type VectorStoreFileBatch as VectorStoreFileBatch,\r\n    type FileBatchCreateParams as FileBatchCreateParams,\r\n    type FileBatchRetrieveParams as FileBatchRetrieveParams,\r\n    type FileBatchCancelParams as FileBatchCancelParams,\r\n    type FileBatchListFilesParams as FileBatchListFilesParams,\r\n  };\r\n}\r\n\r\nexport { type VectorStoreFilesPage };\r\n"],"mappings":"AAAA;SAESA,WAAW,QAAE;SAKbC,UAAU,QAAsC;SAChDC,YAAY,QAAE;SAEdC,KAAK,QAAE;SAEPC,mBAAmB,QAAE;SACrBC,IAAI,QAAE;AAEf,OAAM,MAAOC,WAAY,SAAQN,WAAW;EAC1C;;;EAGAO,MAAMA,CACJC,aAAqB,EACrBC,IAA2B,EAC3BC,OAAwB;IAExB,OAAO,IAAI,CAACC,OAAO,CAACC,IAAI,CAACP,IAAI,kBAAkBG,aAAa,eAAe,EAAE;MAC3EC,IAAI;MACJ,GAAGC,OAAO;MACVG,OAAO,EAAEX,YAAY,CAAC,CAAC;QAAE,aAAa,EAAE;MAAe,CAAE,EAAEQ,OAAO,EAAEG,OAAO,CAAC;KAC7E,CAAC;EACJ;EAEA;;;EAGAC,QAAQA,CACNC,OAAe,EACfC,MAA+B,EAC/BN,OAAwB;IAExB,MAAM;MAAEO;IAAe,CAAE,GAAGD,MAAM;IAClC,OAAO,IAAI,CAACL,OAAO,CAACO,GAAG,CAACb,IAAI,kBAAkBY,eAAe,iBAAiBF,OAAO,EAAE,EAAE;MACvF,GAAGL,OAAO;MACVG,OAAO,EAAEX,YAAY,CAAC,CAAC;QAAE,aAAa,EAAE;MAAe,CAAE,EAAEQ,OAAO,EAAEG,OAAO,CAAC;KAC7E,CAAC;EACJ;EAEA;;;;EAIAM,MAAMA,CACJJ,OAAe,EACfC,MAA6B,EAC7BN,OAAwB;IAExB,MAAM;MAAEO;IAAe,CAAE,GAAGD,MAAM;IAClC,OAAO,IAAI,CAACL,OAAO,CAACC,IAAI,CAACP,IAAI,kBAAkBY,eAAe,iBAAiBF,OAAO,SAAS,EAAE;MAC/F,GAAGL,OAAO;MACVG,OAAO,EAAEX,YAAY,CAAC,CAAC;QAAE,aAAa,EAAE;MAAe,CAAE,EAAEQ,OAAO,EAAEG,OAAO,CAAC;KAC7E,CAAC;EACJ;EAEA;;;EAGA,MAAMO,aAAaA,CACjBC,aAAqB,EACrBZ,IAA2B,EAC3BC,OAAsD;IAEtD,MAAMY,KAAK,GAAG,MAAM,IAAI,CAACf,MAAM,CAACc,aAAa,EAAEZ,IAAI,CAAC;IACpD,OAAO,MAAM,IAAI,CAACc,IAAI,CAACF,aAAa,EAAEC,KAAK,CAACE,EAAE,EAAEd,OAAO,CAAC;EAC1D;EAEA;;;EAGAe,SAASA,CACPV,OAAe,EACfC,MAAgC,EAChCN,OAAwB;IAExB,MAAM;MAAEO,eAAe;MAAE,GAAGS;IAAK,CAAE,GAAGV,MAAM;IAC5C,OAAO,IAAI,CAACL,OAAO,CAACgB,UAAU,CAC5BtB,IAAI,kBAAkBY,eAAe,iBAAiBF,OAAO,QAAQ,EACrEd,UAAoC,EACpC;MAAEyB,KAAK;MAAE,GAAGhB,OAAO;MAAEG,OAAO,EAAEX,YAAY,CAAC,CAAC;QAAE,aAAa,EAAE;MAAe,CAAE,EAAEQ,OAAO,EAAEG,OAAO,CAAC;IAAC,CAAE,CACrG;EACH;EAEA;;;;;;EAMA,MAAMU,IAAIA,CACRf,aAAqB,EACrBO,OAAe,EACfL,OAAsD;IAEtD,MAAMG,OAAO,GAAGX,YAAY,CAAC,CAC3BQ,OAAO,EAAEG,OAAO,EAChB;MACE,yBAAyB,EAAE,MAAM;MACjC,kCAAkC,EAAEH,OAAO,EAAEkB,cAAc,EAAEC,QAAQ,EAAE,IAAIC;KAC5E,CACF,CAAC;IAEF,OAAO,IAAI,EAAE;MACX,MAAM;QAAEC,IAAI,EAAET,KAAK;QAAEU;MAAQ,CAAE,GAAG,MAAM,IAAI,CAAClB,QAAQ,CACnDC,OAAO,EACP;QAAEE,eAAe,EAAET;MAAa,CAAE,EAClC;QACE,GAAGE,OAAO;QACVG;OACD,CACF,CAACoB,YAAY,EAAE;MAEhB,QAAQX,KAAK,CAACY,MAAM;QAClB,KAAK,aAAa;UAChB,IAAIC,aAAa,GAAG,IAAI;UAExB,IAAIzB,OAAO,EAAEkB,cAAc,EAAE;YAC3BO,aAAa,GAAGzB,OAAO,CAACkB,cAAc;UACxC,CAAC,MAAM;YACL,MAAMQ,cAAc,GAAGJ,QAAQ,CAACnB,OAAO,CAACK,GAAG,CAAC,sBAAsB,CAAC;YACnE,IAAIkB,cAAc,EAAE;cAClB,MAAMC,gBAAgB,GAAGC,QAAQ,CAACF,cAAc,CAAC;cACjD,IAAI,CAACG,KAAK,CAACF,gBAAgB,CAAC,EAAE;gBAC5BF,aAAa,GAAGE,gBAAgB;cAClC;YACF;UACF;UACA,MAAMlC,KAAK,CAACgC,aAAa,CAAC;UAC1B;QACF,KAAK,QAAQ;QACb,KAAK,WAAW;QAChB,KAAK,WAAW;UACd,OAAOb,KAAK;MAChB;IACF;EACF;EAEA;;;;;EAKA,MAAMkB,aAAaA,CACjBnB,aAAqB,EACrB;IAAEoB,KAAK;IAAEC,OAAO,GAAG;EAAE,CAA+C,EACpEhC,OAA+E;IAE/E,IAAI+B,KAAK,IAAI,IAAI,IAAIA,KAAK,CAACE,MAAM,IAAI,CAAC,EAAE;MACtC,MAAM,IAAIC,KAAK,CACb,gHAAgH,CACjH;IACH;IAEA,MAAMC,qBAAqB,GAAGnC,OAAO,EAAEoC,cAAc,IAAI,CAAC;IAE1D;IACA,MAAMC,gBAAgB,GAAGC,IAAI,CAACC,GAAG,CAACJ,qBAAqB,EAAEJ,KAAK,CAACE,MAAM,CAAC;IAEtE,MAAMO,MAAM,GAAG,IAAI,CAACvC,OAAO;IAC3B,MAAMwC,YAAY,GAAGV,KAAK,CAACW,MAAM,EAAE;IACnC,MAAMC,UAAU,GAAa,CAAC,GAAGX,OAAO,CAAC;IAEzC;IACA;IACA,eAAeY,YAAYA,CAACC,QAAsC;MAChE,KAAK,IAAIC,IAAI,IAAID,QAAQ,EAAE;QACzB,MAAME,OAAO,GAAG,MAAMP,MAAM,CAACT,KAAK,CAAClC,MAAM,CAAC;UAAEmD,IAAI,EAAEF,IAAI;UAAEG,OAAO,EAAE;QAAY,CAAE,EAAEjD,OAAO,CAAC;QACzF2C,UAAU,CAACO,IAAI,CAACH,OAAO,CAACjC,EAAE,CAAC;MAC7B;IACF;IAEA;IACA,MAAMqC,OAAO,GAAGC,KAAK,CAACf,gBAAgB,CAAC,CAACgB,IAAI,CAACZ,YAAY,CAAC,CAACa,GAAG,CAACV,YAAY,CAAC;IAE5E;IACA,MAAMlD,mBAAmB,CAACyD,OAAO,CAAC;IAElC,OAAO,MAAM,IAAI,CAACzC,aAAa,CAACC,aAAa,EAAE;MAC7C4C,QAAQ,EAAEZ;KACX,CAAC;EACJ","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}