{"ast":null,"code":"// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\nimport { APIResource } from \"../../../core/resource.mjs\";\nimport { buildHeaders } from \"../../../internal/headers.mjs\";\nexport class Sessions extends APIResource {\n  /**\r\n   * Create an ephemeral API token for use in client-side applications with the\r\n   * Realtime API. Can be configured with the same session parameters as the\r\n   * `session.update` client event.\r\n   *\r\n   * It responds with a session object, plus a `client_secret` key which contains a\r\n   * usable ephemeral API token that can be used to authenticate browser clients for\r\n   * the Realtime API.\r\n   *\r\n   * @example\r\n   * ```ts\r\n   * const session =\r\n   *   await client.beta.realtime.sessions.create();\r\n   * ```\r\n   */\n  create(body, options) {\n    return this._client.post('/realtime/sessions', {\n      body,\n      ...options,\n      headers: buildHeaders([{\n        'OpenAI-Beta': 'assistants=v2'\n      }, options?.headers])\n    });\n  }\n}","map":{"version":3,"names":["APIResource","buildHeaders","Sessions","create","body","options","_client","post","headers"],"sources":["C:\\boodschappenlijst\\boodschappenlijst-nieuw\\node_modules\\openai\\src\\resources\\beta\\realtime\\sessions.ts"],"sourcesContent":["// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\r\n\r\nimport { APIResource } from '../../../core/resource';\r\nimport { APIPromise } from '../../../core/api-promise';\r\nimport { buildHeaders } from '../../../internal/headers';\r\nimport { RequestOptions } from '../../../internal/request-options';\r\n\r\nexport class Sessions extends APIResource {\r\n  /**\r\n   * Create an ephemeral API token for use in client-side applications with the\r\n   * Realtime API. Can be configured with the same session parameters as the\r\n   * `session.update` client event.\r\n   *\r\n   * It responds with a session object, plus a `client_secret` key which contains a\r\n   * usable ephemeral API token that can be used to authenticate browser clients for\r\n   * the Realtime API.\r\n   *\r\n   * @example\r\n   * ```ts\r\n   * const session =\r\n   *   await client.beta.realtime.sessions.create();\r\n   * ```\r\n   */\r\n  create(body: SessionCreateParams, options?: RequestOptions): APIPromise<SessionCreateResponse> {\r\n    return this._client.post('/realtime/sessions', {\r\n      body,\r\n      ...options,\r\n      headers: buildHeaders([{ 'OpenAI-Beta': 'assistants=v2' }, options?.headers]),\r\n    });\r\n  }\r\n}\r\n\r\n/**\r\n * Realtime session object configuration.\r\n */\r\nexport interface Session {\r\n  /**\r\n   * Unique identifier for the session that looks like `sess_1234567890abcdef`.\r\n   */\r\n  id?: string;\r\n\r\n  /**\r\n   * The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`. For\r\n   * `pcm16`, input audio must be 16-bit PCM at a 24kHz sample rate, single channel\r\n   * (mono), and little-endian byte order.\r\n   */\r\n  input_audio_format?: 'pcm16' | 'g711_ulaw' | 'g711_alaw';\r\n\r\n  /**\r\n   * Configuration for input audio noise reduction. This can be set to `null` to turn\r\n   * off. Noise reduction filters audio added to the input audio buffer before it is\r\n   * sent to VAD and the model. Filtering the audio can improve VAD and turn\r\n   * detection accuracy (reducing false positives) and model performance by improving\r\n   * perception of the input audio.\r\n   */\r\n  input_audio_noise_reduction?: Session.InputAudioNoiseReduction;\r\n\r\n  /**\r\n   * Configuration for input audio transcription, defaults to off and can be set to\r\n   * `null` to turn off once on. Input audio transcription is not native to the\r\n   * model, since the model consumes audio directly. Transcription runs\r\n   * asynchronously through\r\n   * [the /audio/transcriptions endpoint](https://platform.openai.com/docs/api-reference/audio/createTranscription)\r\n   * and should be treated as guidance of input audio content rather than precisely\r\n   * what the model heard. The client can optionally set the language and prompt for\r\n   * transcription, these offer additional guidance to the transcription service.\r\n   */\r\n  input_audio_transcription?: Session.InputAudioTranscription;\r\n\r\n  /**\r\n   * The default system instructions (i.e. system message) prepended to model calls.\r\n   * This field allows the client to guide the model on desired responses. The model\r\n   * can be instructed on response content and format, (e.g. \"be extremely succinct\",\r\n   * \"act friendly\", \"here are examples of good responses\") and on audio behavior\r\n   * (e.g. \"talk quickly\", \"inject emotion into your voice\", \"laugh frequently\"). The\r\n   * instructions are not guaranteed to be followed by the model, but they provide\r\n   * guidance to the model on the desired behavior.\r\n   *\r\n   * Note that the server sets default instructions which will be used if this field\r\n   * is not set and are visible in the `session.created` event at the start of the\r\n   * session.\r\n   */\r\n  instructions?: string;\r\n\r\n  /**\r\n   * Maximum number of output tokens for a single assistant response, inclusive of\r\n   * tool calls. Provide an integer between 1 and 4096 to limit output tokens, or\r\n   * `inf` for the maximum available tokens for a given model. Defaults to `inf`.\r\n   */\r\n  max_response_output_tokens?: number | 'inf';\r\n\r\n  /**\r\n   * The set of modalities the model can respond with. To disable audio, set this to\r\n   * [\"text\"].\r\n   */\r\n  modalities?: Array<'text' | 'audio'>;\r\n\r\n  /**\r\n   * The Realtime model used for this session.\r\n   */\r\n  model?:\r\n    | 'gpt-4o-realtime-preview'\r\n    | 'gpt-4o-realtime-preview-2024-10-01'\r\n    | 'gpt-4o-realtime-preview-2024-12-17'\r\n    | 'gpt-4o-realtime-preview-2025-06-03'\r\n    | 'gpt-4o-mini-realtime-preview'\r\n    | 'gpt-4o-mini-realtime-preview-2024-12-17';\r\n\r\n  /**\r\n   * The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\r\n   * For `pcm16`, output audio is sampled at a rate of 24kHz.\r\n   */\r\n  output_audio_format?: 'pcm16' | 'g711_ulaw' | 'g711_alaw';\r\n\r\n  /**\r\n   * The speed of the model's spoken response. 1.0 is the default speed. 0.25 is the\r\n   * minimum speed. 1.5 is the maximum speed. This value can only be changed in\r\n   * between model turns, not while a response is in progress.\r\n   */\r\n  speed?: number;\r\n\r\n  /**\r\n   * Sampling temperature for the model, limited to [0.6, 1.2]. For audio models a\r\n   * temperature of 0.8 is highly recommended for best performance.\r\n   */\r\n  temperature?: number;\r\n\r\n  /**\r\n   * How the model chooses tools. Options are `auto`, `none`, `required`, or specify\r\n   * a function.\r\n   */\r\n  tool_choice?: string;\r\n\r\n  /**\r\n   * Tools (functions) available to the model.\r\n   */\r\n  tools?: Array<Session.Tool>;\r\n\r\n  /**\r\n   * Configuration options for tracing. Set to null to disable tracing. Once tracing\r\n   * is enabled for a session, the configuration cannot be modified.\r\n   *\r\n   * `auto` will create a trace for the session with default values for the workflow\r\n   * name, group id, and metadata.\r\n   */\r\n  tracing?: 'auto' | Session.TracingConfiguration;\r\n\r\n  /**\r\n   * Configuration for turn detection, ether Server VAD or Semantic VAD. This can be\r\n   * set to `null` to turn off, in which case the client must manually trigger model\r\n   * response. Server VAD means that the model will detect the start and end of\r\n   * speech based on audio volume and respond at the end of user speech. Semantic VAD\r\n   * is more advanced and uses a turn detection model (in conjuction with VAD) to\r\n   * semantically estimate whether the user has finished speaking, then dynamically\r\n   * sets a timeout based on this probability. For example, if user audio trails off\r\n   * with \"uhhm\", the model will score a low probability of turn end and wait longer\r\n   * for the user to continue speaking. This can be useful for more natural\r\n   * conversations, but may have a higher latency.\r\n   */\r\n  turn_detection?: Session.TurnDetection;\r\n\r\n  /**\r\n   * The voice the model uses to respond. Voice cannot be changed during the session\r\n   * once the model has responded with audio at least once. Current voice options are\r\n   * `alloy`, `ash`, `ballad`, `coral`, `echo`, `fable`, `onyx`, `nova`, `sage`,\r\n   * `shimmer`, and `verse`.\r\n   */\r\n  voice?:\r\n    | (string & {})\r\n    | 'alloy'\r\n    | 'ash'\r\n    | 'ballad'\r\n    | 'coral'\r\n    | 'echo'\r\n    | 'fable'\r\n    | 'onyx'\r\n    | 'nova'\r\n    | 'sage'\r\n    | 'shimmer'\r\n    | 'verse';\r\n}\r\n\r\nexport namespace Session {\r\n  /**\r\n   * Configuration for input audio noise reduction. This can be set to `null` to turn\r\n   * off. Noise reduction filters audio added to the input audio buffer before it is\r\n   * sent to VAD and the model. Filtering the audio can improve VAD and turn\r\n   * detection accuracy (reducing false positives) and model performance by improving\r\n   * perception of the input audio.\r\n   */\r\n  export interface InputAudioNoiseReduction {\r\n    /**\r\n     * Type of noise reduction. `near_field` is for close-talking microphones such as\r\n     * headphones, `far_field` is for far-field microphones such as laptop or\r\n     * conference room microphones.\r\n     */\r\n    type?: 'near_field' | 'far_field';\r\n  }\r\n\r\n  /**\r\n   * Configuration for input audio transcription, defaults to off and can be set to\r\n   * `null` to turn off once on. Input audio transcription is not native to the\r\n   * model, since the model consumes audio directly. Transcription runs\r\n   * asynchronously through\r\n   * [the /audio/transcriptions endpoint](https://platform.openai.com/docs/api-reference/audio/createTranscription)\r\n   * and should be treated as guidance of input audio content rather than precisely\r\n   * what the model heard. The client can optionally set the language and prompt for\r\n   * transcription, these offer additional guidance to the transcription service.\r\n   */\r\n  export interface InputAudioTranscription {\r\n    /**\r\n     * The language of the input audio. Supplying the input language in\r\n     * [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`)\r\n     * format will improve accuracy and latency.\r\n     */\r\n    language?: string;\r\n\r\n    /**\r\n     * The model to use for transcription, current options are `gpt-4o-transcribe`,\r\n     * `gpt-4o-mini-transcribe`, and `whisper-1`.\r\n     */\r\n    model?: string;\r\n\r\n    /**\r\n     * An optional text to guide the model's style or continue a previous audio\r\n     * segment. For `whisper-1`, the\r\n     * [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\r\n     * For `gpt-4o-transcribe` models, the prompt is a free text string, for example\r\n     * \"expect words related to technology\".\r\n     */\r\n    prompt?: string;\r\n  }\r\n\r\n  export interface Tool {\r\n    /**\r\n     * The description of the function, including guidance on when and how to call it,\r\n     * and guidance about what to tell the user when calling (if anything).\r\n     */\r\n    description?: string;\r\n\r\n    /**\r\n     * The name of the function.\r\n     */\r\n    name?: string;\r\n\r\n    /**\r\n     * Parameters of the function in JSON Schema.\r\n     */\r\n    parameters?: unknown;\r\n\r\n    /**\r\n     * The type of the tool, i.e. `function`.\r\n     */\r\n    type?: 'function';\r\n  }\r\n\r\n  /**\r\n   * Granular configuration for tracing.\r\n   */\r\n  export interface TracingConfiguration {\r\n    /**\r\n     * The group id to attach to this trace to enable filtering and grouping in the\r\n     * traces dashboard.\r\n     */\r\n    group_id?: string;\r\n\r\n    /**\r\n     * The arbitrary metadata to attach to this trace to enable filtering in the traces\r\n     * dashboard.\r\n     */\r\n    metadata?: unknown;\r\n\r\n    /**\r\n     * The name of the workflow to attach to this trace. This is used to name the trace\r\n     * in the traces dashboard.\r\n     */\r\n    workflow_name?: string;\r\n  }\r\n\r\n  /**\r\n   * Configuration for turn detection, ether Server VAD or Semantic VAD. This can be\r\n   * set to `null` to turn off, in which case the client must manually trigger model\r\n   * response. Server VAD means that the model will detect the start and end of\r\n   * speech based on audio volume and respond at the end of user speech. Semantic VAD\r\n   * is more advanced and uses a turn detection model (in conjuction with VAD) to\r\n   * semantically estimate whether the user has finished speaking, then dynamically\r\n   * sets a timeout based on this probability. For example, if user audio trails off\r\n   * with \"uhhm\", the model will score a low probability of turn end and wait longer\r\n   * for the user to continue speaking. This can be useful for more natural\r\n   * conversations, but may have a higher latency.\r\n   */\r\n  export interface TurnDetection {\r\n    /**\r\n     * Whether or not to automatically generate a response when a VAD stop event\r\n     * occurs.\r\n     */\r\n    create_response?: boolean;\r\n\r\n    /**\r\n     * Used only for `semantic_vad` mode. The eagerness of the model to respond. `low`\r\n     * will wait longer for the user to continue speaking, `high` will respond more\r\n     * quickly. `auto` is the default and is equivalent to `medium`.\r\n     */\r\n    eagerness?: 'low' | 'medium' | 'high' | 'auto';\r\n\r\n    /**\r\n     * Whether or not to automatically interrupt any ongoing response with output to\r\n     * the default conversation (i.e. `conversation` of `auto`) when a VAD start event\r\n     * occurs.\r\n     */\r\n    interrupt_response?: boolean;\r\n\r\n    /**\r\n     * Used only for `server_vad` mode. Amount of audio to include before the VAD\r\n     * detected speech (in milliseconds). Defaults to 300ms.\r\n     */\r\n    prefix_padding_ms?: number;\r\n\r\n    /**\r\n     * Used only for `server_vad` mode. Duration of silence to detect speech stop (in\r\n     * milliseconds). Defaults to 500ms. With shorter values the model will respond\r\n     * more quickly, but may jump in on short pauses from the user.\r\n     */\r\n    silence_duration_ms?: number;\r\n\r\n    /**\r\n     * Used only for `server_vad` mode. Activation threshold for VAD (0.0 to 1.0), this\r\n     * defaults to 0.5. A higher threshold will require louder audio to activate the\r\n     * model, and thus might perform better in noisy environments.\r\n     */\r\n    threshold?: number;\r\n\r\n    /**\r\n     * Type of turn detection.\r\n     */\r\n    type?: 'server_vad' | 'semantic_vad';\r\n  }\r\n}\r\n\r\n/**\r\n * A new Realtime session configuration, with an ephermeral key. Default TTL for\r\n * keys is one minute.\r\n */\r\nexport interface SessionCreateResponse {\r\n  /**\r\n   * Ephemeral key returned by the API.\r\n   */\r\n  client_secret: SessionCreateResponse.ClientSecret;\r\n\r\n  /**\r\n   * The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\r\n   */\r\n  input_audio_format?: string;\r\n\r\n  /**\r\n   * Configuration for input audio transcription, defaults to off and can be set to\r\n   * `null` to turn off once on. Input audio transcription is not native to the\r\n   * model, since the model consumes audio directly. Transcription runs\r\n   * asynchronously through Whisper and should be treated as rough guidance rather\r\n   * than the representation understood by the model.\r\n   */\r\n  input_audio_transcription?: SessionCreateResponse.InputAudioTranscription;\r\n\r\n  /**\r\n   * The default system instructions (i.e. system message) prepended to model calls.\r\n   * This field allows the client to guide the model on desired responses. The model\r\n   * can be instructed on response content and format, (e.g. \"be extremely succinct\",\r\n   * \"act friendly\", \"here are examples of good responses\") and on audio behavior\r\n   * (e.g. \"talk quickly\", \"inject emotion into your voice\", \"laugh frequently\"). The\r\n   * instructions are not guaranteed to be followed by the model, but they provide\r\n   * guidance to the model on the desired behavior.\r\n   *\r\n   * Note that the server sets default instructions which will be used if this field\r\n   * is not set and are visible in the `session.created` event at the start of the\r\n   * session.\r\n   */\r\n  instructions?: string;\r\n\r\n  /**\r\n   * Maximum number of output tokens for a single assistant response, inclusive of\r\n   * tool calls. Provide an integer between 1 and 4096 to limit output tokens, or\r\n   * `inf` for the maximum available tokens for a given model. Defaults to `inf`.\r\n   */\r\n  max_response_output_tokens?: number | 'inf';\r\n\r\n  /**\r\n   * The set of modalities the model can respond with. To disable audio, set this to\r\n   * [\"text\"].\r\n   */\r\n  modalities?: Array<'text' | 'audio'>;\r\n\r\n  /**\r\n   * The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\r\n   */\r\n  output_audio_format?: string;\r\n\r\n  /**\r\n   * The speed of the model's spoken response. 1.0 is the default speed. 0.25 is the\r\n   * minimum speed. 1.5 is the maximum speed. This value can only be changed in\r\n   * between model turns, not while a response is in progress.\r\n   */\r\n  speed?: number;\r\n\r\n  /**\r\n   * Sampling temperature for the model, limited to [0.6, 1.2]. Defaults to 0.8.\r\n   */\r\n  temperature?: number;\r\n\r\n  /**\r\n   * How the model chooses tools. Options are `auto`, `none`, `required`, or specify\r\n   * a function.\r\n   */\r\n  tool_choice?: string;\r\n\r\n  /**\r\n   * Tools (functions) available to the model.\r\n   */\r\n  tools?: Array<SessionCreateResponse.Tool>;\r\n\r\n  /**\r\n   * Configuration options for tracing. Set to null to disable tracing. Once tracing\r\n   * is enabled for a session, the configuration cannot be modified.\r\n   *\r\n   * `auto` will create a trace for the session with default values for the workflow\r\n   * name, group id, and metadata.\r\n   */\r\n  tracing?: 'auto' | SessionCreateResponse.TracingConfiguration;\r\n\r\n  /**\r\n   * Configuration for turn detection. Can be set to `null` to turn off. Server VAD\r\n   * means that the model will detect the start and end of speech based on audio\r\n   * volume and respond at the end of user speech.\r\n   */\r\n  turn_detection?: SessionCreateResponse.TurnDetection;\r\n\r\n  /**\r\n   * The voice the model uses to respond. Voice cannot be changed during the session\r\n   * once the model has responded with audio at least once. Current voice options are\r\n   * `alloy`, `ash`, `ballad`, `coral`, `echo` `sage`, `shimmer` and `verse`.\r\n   */\r\n  voice?:\r\n    | (string & {})\r\n    | 'alloy'\r\n    | 'ash'\r\n    | 'ballad'\r\n    | 'coral'\r\n    | 'echo'\r\n    | 'fable'\r\n    | 'onyx'\r\n    | 'nova'\r\n    | 'sage'\r\n    | 'shimmer'\r\n    | 'verse';\r\n}\r\n\r\nexport namespace SessionCreateResponse {\r\n  /**\r\n   * Ephemeral key returned by the API.\r\n   */\r\n  export interface ClientSecret {\r\n    /**\r\n     * Timestamp for when the token expires. Currently, all tokens expire after one\r\n     * minute.\r\n     */\r\n    expires_at: number;\r\n\r\n    /**\r\n     * Ephemeral key usable in client environments to authenticate connections to the\r\n     * Realtime API. Use this in client-side environments rather than a standard API\r\n     * token, which should only be used server-side.\r\n     */\r\n    value: string;\r\n  }\r\n\r\n  /**\r\n   * Configuration for input audio transcription, defaults to off and can be set to\r\n   * `null` to turn off once on. Input audio transcription is not native to the\r\n   * model, since the model consumes audio directly. Transcription runs\r\n   * asynchronously through Whisper and should be treated as rough guidance rather\r\n   * than the representation understood by the model.\r\n   */\r\n  export interface InputAudioTranscription {\r\n    /**\r\n     * The model to use for transcription, `whisper-1` is the only currently supported\r\n     * model.\r\n     */\r\n    model?: string;\r\n  }\r\n\r\n  export interface Tool {\r\n    /**\r\n     * The description of the function, including guidance on when and how to call it,\r\n     * and guidance about what to tell the user when calling (if anything).\r\n     */\r\n    description?: string;\r\n\r\n    /**\r\n     * The name of the function.\r\n     */\r\n    name?: string;\r\n\r\n    /**\r\n     * Parameters of the function in JSON Schema.\r\n     */\r\n    parameters?: unknown;\r\n\r\n    /**\r\n     * The type of the tool, i.e. `function`.\r\n     */\r\n    type?: 'function';\r\n  }\r\n\r\n  /**\r\n   * Granular configuration for tracing.\r\n   */\r\n  export interface TracingConfiguration {\r\n    /**\r\n     * The group id to attach to this trace to enable filtering and grouping in the\r\n     * traces dashboard.\r\n     */\r\n    group_id?: string;\r\n\r\n    /**\r\n     * The arbitrary metadata to attach to this trace to enable filtering in the traces\r\n     * dashboard.\r\n     */\r\n    metadata?: unknown;\r\n\r\n    /**\r\n     * The name of the workflow to attach to this trace. This is used to name the trace\r\n     * in the traces dashboard.\r\n     */\r\n    workflow_name?: string;\r\n  }\r\n\r\n  /**\r\n   * Configuration for turn detection. Can be set to `null` to turn off. Server VAD\r\n   * means that the model will detect the start and end of speech based on audio\r\n   * volume and respond at the end of user speech.\r\n   */\r\n  export interface TurnDetection {\r\n    /**\r\n     * Amount of audio to include before the VAD detected speech (in milliseconds).\r\n     * Defaults to 300ms.\r\n     */\r\n    prefix_padding_ms?: number;\r\n\r\n    /**\r\n     * Duration of silence to detect speech stop (in milliseconds). Defaults to 500ms.\r\n     * With shorter values the model will respond more quickly, but may jump in on\r\n     * short pauses from the user.\r\n     */\r\n    silence_duration_ms?: number;\r\n\r\n    /**\r\n     * Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A higher\r\n     * threshold will require louder audio to activate the model, and thus might\r\n     * perform better in noisy environments.\r\n     */\r\n    threshold?: number;\r\n\r\n    /**\r\n     * Type of turn detection, only `server_vad` is currently supported.\r\n     */\r\n    type?: string;\r\n  }\r\n}\r\n\r\nexport interface SessionCreateParams {\r\n  /**\r\n   * Configuration options for the generated client secret.\r\n   */\r\n  client_secret?: SessionCreateParams.ClientSecret;\r\n\r\n  /**\r\n   * The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`. For\r\n   * `pcm16`, input audio must be 16-bit PCM at a 24kHz sample rate, single channel\r\n   * (mono), and little-endian byte order.\r\n   */\r\n  input_audio_format?: 'pcm16' | 'g711_ulaw' | 'g711_alaw';\r\n\r\n  /**\r\n   * Configuration for input audio noise reduction. This can be set to `null` to turn\r\n   * off. Noise reduction filters audio added to the input audio buffer before it is\r\n   * sent to VAD and the model. Filtering the audio can improve VAD and turn\r\n   * detection accuracy (reducing false positives) and model performance by improving\r\n   * perception of the input audio.\r\n   */\r\n  input_audio_noise_reduction?: SessionCreateParams.InputAudioNoiseReduction;\r\n\r\n  /**\r\n   * Configuration for input audio transcription, defaults to off and can be set to\r\n   * `null` to turn off once on. Input audio transcription is not native to the\r\n   * model, since the model consumes audio directly. Transcription runs\r\n   * asynchronously through\r\n   * [the /audio/transcriptions endpoint](https://platform.openai.com/docs/api-reference/audio/createTranscription)\r\n   * and should be treated as guidance of input audio content rather than precisely\r\n   * what the model heard. The client can optionally set the language and prompt for\r\n   * transcription, these offer additional guidance to the transcription service.\r\n   */\r\n  input_audio_transcription?: SessionCreateParams.InputAudioTranscription;\r\n\r\n  /**\r\n   * The default system instructions (i.e. system message) prepended to model calls.\r\n   * This field allows the client to guide the model on desired responses. The model\r\n   * can be instructed on response content and format, (e.g. \"be extremely succinct\",\r\n   * \"act friendly\", \"here are examples of good responses\") and on audio behavior\r\n   * (e.g. \"talk quickly\", \"inject emotion into your voice\", \"laugh frequently\"). The\r\n   * instructions are not guaranteed to be followed by the model, but they provide\r\n   * guidance to the model on the desired behavior.\r\n   *\r\n   * Note that the server sets default instructions which will be used if this field\r\n   * is not set and are visible in the `session.created` event at the start of the\r\n   * session.\r\n   */\r\n  instructions?: string;\r\n\r\n  /**\r\n   * Maximum number of output tokens for a single assistant response, inclusive of\r\n   * tool calls. Provide an integer between 1 and 4096 to limit output tokens, or\r\n   * `inf` for the maximum available tokens for a given model. Defaults to `inf`.\r\n   */\r\n  max_response_output_tokens?: number | 'inf';\r\n\r\n  /**\r\n   * The set of modalities the model can respond with. To disable audio, set this to\r\n   * [\"text\"].\r\n   */\r\n  modalities?: Array<'text' | 'audio'>;\r\n\r\n  /**\r\n   * The Realtime model used for this session.\r\n   */\r\n  model?:\r\n    | 'gpt-4o-realtime-preview'\r\n    | 'gpt-4o-realtime-preview-2024-10-01'\r\n    | 'gpt-4o-realtime-preview-2024-12-17'\r\n    | 'gpt-4o-realtime-preview-2025-06-03'\r\n    | 'gpt-4o-mini-realtime-preview'\r\n    | 'gpt-4o-mini-realtime-preview-2024-12-17';\r\n\r\n  /**\r\n   * The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\r\n   * For `pcm16`, output audio is sampled at a rate of 24kHz.\r\n   */\r\n  output_audio_format?: 'pcm16' | 'g711_ulaw' | 'g711_alaw';\r\n\r\n  /**\r\n   * The speed of the model's spoken response. 1.0 is the default speed. 0.25 is the\r\n   * minimum speed. 1.5 is the maximum speed. This value can only be changed in\r\n   * between model turns, not while a response is in progress.\r\n   */\r\n  speed?: number;\r\n\r\n  /**\r\n   * Sampling temperature for the model, limited to [0.6, 1.2]. For audio models a\r\n   * temperature of 0.8 is highly recommended for best performance.\r\n   */\r\n  temperature?: number;\r\n\r\n  /**\r\n   * How the model chooses tools. Options are `auto`, `none`, `required`, or specify\r\n   * a function.\r\n   */\r\n  tool_choice?: string;\r\n\r\n  /**\r\n   * Tools (functions) available to the model.\r\n   */\r\n  tools?: Array<SessionCreateParams.Tool>;\r\n\r\n  /**\r\n   * Configuration options for tracing. Set to null to disable tracing. Once tracing\r\n   * is enabled for a session, the configuration cannot be modified.\r\n   *\r\n   * `auto` will create a trace for the session with default values for the workflow\r\n   * name, group id, and metadata.\r\n   */\r\n  tracing?: 'auto' | SessionCreateParams.TracingConfiguration;\r\n\r\n  /**\r\n   * Configuration for turn detection, ether Server VAD or Semantic VAD. This can be\r\n   * set to `null` to turn off, in which case the client must manually trigger model\r\n   * response. Server VAD means that the model will detect the start and end of\r\n   * speech based on audio volume and respond at the end of user speech. Semantic VAD\r\n   * is more advanced and uses a turn detection model (in conjuction with VAD) to\r\n   * semantically estimate whether the user has finished speaking, then dynamically\r\n   * sets a timeout based on this probability. For example, if user audio trails off\r\n   * with \"uhhm\", the model will score a low probability of turn end and wait longer\r\n   * for the user to continue speaking. This can be useful for more natural\r\n   * conversations, but may have a higher latency.\r\n   */\r\n  turn_detection?: SessionCreateParams.TurnDetection;\r\n\r\n  /**\r\n   * The voice the model uses to respond. Voice cannot be changed during the session\r\n   * once the model has responded with audio at least once. Current voice options are\r\n   * `alloy`, `ash`, `ballad`, `coral`, `echo`, `fable`, `onyx`, `nova`, `sage`,\r\n   * `shimmer`, and `verse`.\r\n   */\r\n  voice?:\r\n    | (string & {})\r\n    | 'alloy'\r\n    | 'ash'\r\n    | 'ballad'\r\n    | 'coral'\r\n    | 'echo'\r\n    | 'fable'\r\n    | 'onyx'\r\n    | 'nova'\r\n    | 'sage'\r\n    | 'shimmer'\r\n    | 'verse';\r\n}\r\n\r\nexport namespace SessionCreateParams {\r\n  /**\r\n   * Configuration options for the generated client secret.\r\n   */\r\n  export interface ClientSecret {\r\n    /**\r\n     * Configuration for the ephemeral token expiration.\r\n     */\r\n    expires_at?: ClientSecret.ExpiresAt;\r\n  }\r\n\r\n  export namespace ClientSecret {\r\n    /**\r\n     * Configuration for the ephemeral token expiration.\r\n     */\r\n    export interface ExpiresAt {\r\n      /**\r\n       * The anchor point for the ephemeral token expiration. Only `created_at` is\r\n       * currently supported.\r\n       */\r\n      anchor?: 'created_at';\r\n\r\n      /**\r\n       * The number of seconds from the anchor point to the expiration. Select a value\r\n       * between `10` and `7200`.\r\n       */\r\n      seconds?: number;\r\n    }\r\n  }\r\n\r\n  /**\r\n   * Configuration for input audio noise reduction. This can be set to `null` to turn\r\n   * off. Noise reduction filters audio added to the input audio buffer before it is\r\n   * sent to VAD and the model. Filtering the audio can improve VAD and turn\r\n   * detection accuracy (reducing false positives) and model performance by improving\r\n   * perception of the input audio.\r\n   */\r\n  export interface InputAudioNoiseReduction {\r\n    /**\r\n     * Type of noise reduction. `near_field` is for close-talking microphones such as\r\n     * headphones, `far_field` is for far-field microphones such as laptop or\r\n     * conference room microphones.\r\n     */\r\n    type?: 'near_field' | 'far_field';\r\n  }\r\n\r\n  /**\r\n   * Configuration for input audio transcription, defaults to off and can be set to\r\n   * `null` to turn off once on. Input audio transcription is not native to the\r\n   * model, since the model consumes audio directly. Transcription runs\r\n   * asynchronously through\r\n   * [the /audio/transcriptions endpoint](https://platform.openai.com/docs/api-reference/audio/createTranscription)\r\n   * and should be treated as guidance of input audio content rather than precisely\r\n   * what the model heard. The client can optionally set the language and prompt for\r\n   * transcription, these offer additional guidance to the transcription service.\r\n   */\r\n  export interface InputAudioTranscription {\r\n    /**\r\n     * The language of the input audio. Supplying the input language in\r\n     * [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`)\r\n     * format will improve accuracy and latency.\r\n     */\r\n    language?: string;\r\n\r\n    /**\r\n     * The model to use for transcription, current options are `gpt-4o-transcribe`,\r\n     * `gpt-4o-mini-transcribe`, and `whisper-1`.\r\n     */\r\n    model?: string;\r\n\r\n    /**\r\n     * An optional text to guide the model's style or continue a previous audio\r\n     * segment. For `whisper-1`, the\r\n     * [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\r\n     * For `gpt-4o-transcribe` models, the prompt is a free text string, for example\r\n     * \"expect words related to technology\".\r\n     */\r\n    prompt?: string;\r\n  }\r\n\r\n  export interface Tool {\r\n    /**\r\n     * The description of the function, including guidance on when and how to call it,\r\n     * and guidance about what to tell the user when calling (if anything).\r\n     */\r\n    description?: string;\r\n\r\n    /**\r\n     * The name of the function.\r\n     */\r\n    name?: string;\r\n\r\n    /**\r\n     * Parameters of the function in JSON Schema.\r\n     */\r\n    parameters?: unknown;\r\n\r\n    /**\r\n     * The type of the tool, i.e. `function`.\r\n     */\r\n    type?: 'function';\r\n  }\r\n\r\n  /**\r\n   * Granular configuration for tracing.\r\n   */\r\n  export interface TracingConfiguration {\r\n    /**\r\n     * The group id to attach to this trace to enable filtering and grouping in the\r\n     * traces dashboard.\r\n     */\r\n    group_id?: string;\r\n\r\n    /**\r\n     * The arbitrary metadata to attach to this trace to enable filtering in the traces\r\n     * dashboard.\r\n     */\r\n    metadata?: unknown;\r\n\r\n    /**\r\n     * The name of the workflow to attach to this trace. This is used to name the trace\r\n     * in the traces dashboard.\r\n     */\r\n    workflow_name?: string;\r\n  }\r\n\r\n  /**\r\n   * Configuration for turn detection, ether Server VAD or Semantic VAD. This can be\r\n   * set to `null` to turn off, in which case the client must manually trigger model\r\n   * response. Server VAD means that the model will detect the start and end of\r\n   * speech based on audio volume and respond at the end of user speech. Semantic VAD\r\n   * is more advanced and uses a turn detection model (in conjuction with VAD) to\r\n   * semantically estimate whether the user has finished speaking, then dynamically\r\n   * sets a timeout based on this probability. For example, if user audio trails off\r\n   * with \"uhhm\", the model will score a low probability of turn end and wait longer\r\n   * for the user to continue speaking. This can be useful for more natural\r\n   * conversations, but may have a higher latency.\r\n   */\r\n  export interface TurnDetection {\r\n    /**\r\n     * Whether or not to automatically generate a response when a VAD stop event\r\n     * occurs.\r\n     */\r\n    create_response?: boolean;\r\n\r\n    /**\r\n     * Used only for `semantic_vad` mode. The eagerness of the model to respond. `low`\r\n     * will wait longer for the user to continue speaking, `high` will respond more\r\n     * quickly. `auto` is the default and is equivalent to `medium`.\r\n     */\r\n    eagerness?: 'low' | 'medium' | 'high' | 'auto';\r\n\r\n    /**\r\n     * Whether or not to automatically interrupt any ongoing response with output to\r\n     * the default conversation (i.e. `conversation` of `auto`) when a VAD start event\r\n     * occurs.\r\n     */\r\n    interrupt_response?: boolean;\r\n\r\n    /**\r\n     * Used only for `server_vad` mode. Amount of audio to include before the VAD\r\n     * detected speech (in milliseconds). Defaults to 300ms.\r\n     */\r\n    prefix_padding_ms?: number;\r\n\r\n    /**\r\n     * Used only for `server_vad` mode. Duration of silence to detect speech stop (in\r\n     * milliseconds). Defaults to 500ms. With shorter values the model will respond\r\n     * more quickly, but may jump in on short pauses from the user.\r\n     */\r\n    silence_duration_ms?: number;\r\n\r\n    /**\r\n     * Used only for `server_vad` mode. Activation threshold for VAD (0.0 to 1.0), this\r\n     * defaults to 0.5. A higher threshold will require louder audio to activate the\r\n     * model, and thus might perform better in noisy environments.\r\n     */\r\n    threshold?: number;\r\n\r\n    /**\r\n     * Type of turn detection.\r\n     */\r\n    type?: 'server_vad' | 'semantic_vad';\r\n  }\r\n}\r\n\r\nexport declare namespace Sessions {\r\n  export {\r\n    type Session as Session,\r\n    type SessionCreateResponse as SessionCreateResponse,\r\n    type SessionCreateParams as SessionCreateParams,\r\n  };\r\n}\r\n"],"mappings":"AAAA;SAESA,WAAW,QAAE;SAEbC,YAAY,QAAE;AAGvB,OAAM,MAAOC,QAAS,SAAQF,WAAW;EACvC;;;;;;;;;;;;;;;EAeAG,MAAMA,CAACC,IAAyB,EAAEC,OAAwB;IACxD,OAAO,IAAI,CAACC,OAAO,CAACC,IAAI,CAAC,oBAAoB,EAAE;MAC7CH,IAAI;MACJ,GAAGC,OAAO;MACVG,OAAO,EAAEP,YAAY,CAAC,CAAC;QAAE,aAAa,EAAE;MAAe,CAAE,EAAEI,OAAO,EAAEG,OAAO,CAAC;KAC7E,CAAC;EACJ","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}